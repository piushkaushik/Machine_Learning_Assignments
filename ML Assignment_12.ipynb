{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74d49c17",
   "metadata": {},
   "source": [
    "## 1. What is prior probability? Give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c690bb46",
   "metadata": {},
   "source": [
    "#### Prior probability:\n",
    "\n",
    "The prior probability of an event will be revised as new data or information becomes available, to produce a more accurate measure of a potential outcome. That revised probability becomes the posterior probability and is calculated using Bayes' theorem. In statistical terms, the posterior probability is the probability of event A occurring given that event B has occurred.\n",
    "\n",
    "\n",
    "#### Example:\n",
    "\n",
    "For example, three acres of land have the labels A, B, and C. One acre has reserves of oil below its surface, while the other two do not. The prior probability of oil being found on acre C is one third, or 0.333. But if a drilling test is conducted on acre B, and the results indicate that no oil is present at the location, then the posterior probability of oil being found on acres A and C become 0.5, as each acre has one out of two chances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2d5189",
   "metadata": {},
   "source": [
    "Bayes' Theorem is expressed as:\n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $P(A)$ = the prior probability of A occurring\n",
    "- $P(B|A)$ = the conditional probability of B given that A occurs\n",
    "- $P(B)$ = the probability of B occurring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f40613",
   "metadata": {},
   "source": [
    "## 2. What is posterior probability? Give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0785c9cb",
   "metadata": {},
   "source": [
    "#### Posterior probability:\n",
    "\n",
    "A posterior probability, in Bayesian statistics, is the revised or updated probability of an event occurring after taking into consideration new information. The posterior probability is calculated by updating the prior probability using Bayes' theorem.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "An example of posterior probability is re-evaluating the probability of a medical diagnosis (e.g., a disease) based on new test results, taking into account prior knowledge and the test's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ced0e7",
   "metadata": {},
   "source": [
    "## 3. What is likelihood probability? Give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d93422",
   "metadata": {},
   "source": [
    "#### Likelihood:\n",
    "\n",
    "The term Likelihood refers to the process of determining the best data distribution given a specific situation in the data.\n",
    "\n",
    "\n",
    "Likelihood is a measure of how well a statistical model or hypothesis explains observed data. It assesses the probability of observing the data given a specific model or hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e122857",
   "metadata": {},
   "source": [
    "#### Example:\n",
    "\n",
    "Suppose you are conducting a coin-flipping experiment. You have two hypotheses or models: Model A suggests the coin is fair (50% chance of heads or tails), and Model B suggests the coin is biased (60% chance of heads, 40% chance of tails).\n",
    "\n",
    "Now, you perform 10 coin flips, and you observe 8 heads and 2 tails. To calculate the likelihood of the data under each model:\n",
    "\n",
    "- Likelihood under Model A: You calculate the probability of getting 8 heads and 2 tails if the coin is fair. This probability is given by the binomial probability formula.\n",
    "- Likelihood under Model B: You calculate the probability of getting 8 heads and 2 tails if the coin is biased as per Model B.\n",
    "\n",
    "In this case, you are assessing how well each model explains the observed data, and the likelihood probability quantifies this explanation. It does not include prior beliefs or probabilities, as Bayesian analysis does; it focuses solely on the observed data and its compatibility with a particular model. The model with the higher likelihood given the data is considered more plausible.\n",
    "\n",
    "**Likelihood measures how well a model or hypothesis fits the observed data, and it plays a fundamental role in statistical modeling and hypothesis testing.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e99341d",
   "metadata": {},
   "source": [
    "## 4. What is Naïve Bayes classifier? Why is it named so?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd7723c",
   "metadata": {},
   "source": [
    "#### Naïve Bayes classifier:\n",
    "\n",
    "In statistics, naive Bayes classifiers are considered as simple probabilistic classifiers that apply Bayes’ theorem. This theorem is based on the probability of a hypothesis, given the data and some prior knowledge. The naive Bayes classifier assumes that all features in the input data are independent of each other, which is often not true in real-world scenarios. However, despite this simplifying assumption, the naive Bayes classifier is widely used because of its efficiency and good performance in many real-world applications.\n",
    "\n",
    "Bayes theorem provides a way of computing posterior probability P(c|x) from P(c), P(x) and P(x|c). Look at the equation below:\n",
    "\n",
    "![Bayes theorem](https://av-eks-blogoptimized.s3.amazonaws.com/Bayes_rule-300x172-300x172-111664.png)\n",
    "\n",
    "\n",
    "\n",
    "Above,\n",
    "\n",
    "- P(c|x) is the posterior probability of class (c, target) given predictor (x, attributes).\n",
    "- P(c) is the prior probability of class.\n",
    "- P(x|c) is the likelihood which is the probability of the predictor given class.\n",
    "- P(x) is the prior probability of the predictor.\n",
    "\n",
    "#### Why is it named so?\n",
    "\n",
    "**It is named \"naïve\" because the naive Bayes classifier assumes that all features in the input data are independent of each other**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69db8d83",
   "metadata": {},
   "source": [
    "## 5. What is optimal Bayes classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610ee14f",
   "metadata": {},
   "source": [
    "The Bayes optimal classifier is a probabilistic model that makes the most probable prediction for a new example, given the training dataset.\n",
    "\n",
    "1. **Modeling the Problem:** To use the Optimal Bayes Classifier, you first need to build a probabilistic model of the data. You estimate the prior probabilities (the probability of each class occurring without any data), and the conditional probabilities (the likelihood of observing particular data for each class).\n",
    "\n",
    "2. **Decision Rule:** The classifier then applies the Bayes' Theorem to calculate the posterior probabilities for each class, given the data. It selects the class with the highest posterior probability as the predicted class.\n",
    "\n",
    "Mathematically, the decision rule for the Optimal Bayes Classifier is:\n",
    "\n",
    "Choose class $C_k$ if:\n",
    "\n",
    "$$P(C_k | \\mathbf{x}) = \\frac{P(\\mathbf{x} | C_k) \\cdot P(C_k)}{P(\\mathbf{x})}$$\n",
    "\n",
    "is maximized over all classes $k$.\n",
    "\n",
    "\n",
    "**Challenge with Optimal Bayes classifier:**\n",
    "\n",
    "\n",
    "The main challenge with the Optimal Bayes Classifier is that it requires knowing the true underlying probability distributions, which is often not the case in practical machine learning applications. Instead, various classification algorithms are used, such as Naive Bayes, Support Vector Machines, or Decision Trees, to make predictions based on the available data. These algorithms approximate the Optimal Bayes Classifier's performance under the given data constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6366c7f2",
   "metadata": {},
   "source": [
    "## 6. Write any two features of Bayesian learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177ac3cf",
   "metadata": {},
   "source": [
    "**1. Probabilistic Framework:** \n",
    "\n",
    "Bayesian methods provide a probabilistic framework for modeling uncertainty. They assign probabilities to different hypotheses or model parameters, allowing for quantification of uncertainty and updating beliefs as new data becomes available.\n",
    "\n",
    "**2. Incorporation of Prior Information:** \n",
    "\n",
    "Bayesian learning allows the incorporation of prior knowledge or beliefs into the modeling process through prior probability distributions. This prior information can influence the model's predictions and is particularly useful when dealing with limited data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18188bff",
   "metadata": {},
   "source": [
    "## 7. Define the concept of consistent learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20709911",
   "metadata": {},
   "source": [
    "**Consistent Learners:**\n",
    "\n",
    "\n",
    "\"Consistent learner\" refers to a type of learning algorithm or model that has a critical property related to its performance and behavior as the amount of data available for training grows. Specifically, a consistent learner is one that converges to the \"true\" or \"population\" model as the size of the training dataset becomes infinitely large. This concept is closely tied to statistical consistency.\n",
    "\n",
    "Here's a more detailed explanation:\n",
    "\n",
    "1. **True Model:** In any machine learning problem, there is a true underlying model that generates the data. For example, if you're working on a regression problem, there's a true function that relates the input variables to the output. Consistency is concerned with how closely the learner's model approaches this true model.\n",
    "\n",
    "2. **Convergence:** A consistent learner is expected to converge to the true model. In other words, as you feed more and more data into the learner, the model it produces becomes increasingly accurate and approaches the true model. This is a desirable property because it means that, given enough data, the learner's predictions will be very close to the actual data-generating process.\n",
    "\n",
    "3. **Asymptotic Behavior:** Consistency often relates to the asymptotic behavior of the learner. Asymptotics involve studying what happens as the sample size approaches infinity.\n",
    "\n",
    "Consistency is a theoretical concept and may not always apply to practical machine learning algorithms. Whether a specific algorithm is consistent depends on various factors, including the learning algorithm itself, the problem's complexity, and the quality and quantity of the data.\n",
    "\n",
    "One common example of consistent learners is the family of Maximum Likelihood Estimators (MLEs). MLEs are consistent estimators because, with a sufficiently large sample size, they converge to the true parameters of the underlying probability distribution.\n",
    "\n",
    "Consistency is a desirable property because it suggests that, given enough data, the learner will make predictions that are arbitrarily close to the best possible predictions, based on the true model. However, in practice, this theoretical idea may be limited by factors like model complexity and the quality of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283eeda7",
   "metadata": {},
   "source": [
    "## 8. Write any two strengths of Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cf7e37",
   "metadata": {},
   "source": [
    "1. Efficient and Scalable: Naïve Bayes is a computationally efficient algorithm that scales well with high-dimensional datasets. It can handle a large number of features, making it suitable for text classification, document categorization, and other tasks involving a multitude of attributes.\n",
    "\n",
    "\n",
    "2. Simple and Effective for Categorical Data: Naïve Bayes is particularly effective for categorical and discrete data. It performs well in situations where feature independence assumptions are reasonable, such as spam detection, sentiment analysis, and recommendation systems, even though these assumptions might not hold in reality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f35ebb",
   "metadata": {},
   "source": [
    "## 9. Write any two weaknesses of Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775f8279",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. Strong Independence Assumption: The classifier assumes that features are conditionally independent, which may not hold in some real-world scenarios. This can lead to suboptimal performance when features are correlated.\n",
    "\n",
    "\n",
    "\n",
    "2. Limited Expressiveness: Naïve Bayes is not well-suited for tasks that require modeling complex relationships and dependencies between features. It may struggle with capturing nuanced patterns in data, especially when the independence assumption is violated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fa5e98",
   "metadata": {},
   "source": [
    "## 10. Explain how Naïve Bayes classifier is used for\n",
    "### 1. Text classification\n",
    "### 2. Spam filtering\n",
    "### 3. Market sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6979a48c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Text Classification:\n",
    "   - In text classification, the goal is to categorize text documents into predefined classes or categories, such as news articles, customer reviews, or social media posts.\n",
    "   - The Naïve Bayes classifier is employed to determine the probability of a document belonging to a specific category based on the words or features present in the document.\n",
    "   - The classifier calculates the conditional probabilities of each word occurring in a document given its category. The product of these probabilities is used to estimate the probability of the document belonging to a category.\n",
    "   - This method is particularly effective for text data because it can efficiently handle a large number of features (words) and is well-suited for high-dimensional data. It's commonly used in applications like spam detection, topic classification, and sentiment analysis.\n",
    "\n",
    "### Spam Filtering:\n",
    "   - Spam filtering aims to identify and filter out unwanted or unsolicited emails from a user's inbox.\n",
    "   - Naïve Bayes is used to assess the probability of an email being spam or not, based on the words, phrases, and patterns in the email content.\n",
    "   - The classifier is trained on a dataset of labeled emails (spam and non-spam) to estimate the conditional probabilities of certain words or features occurring in spam or legitimate emails.\n",
    "   - When a new email arrives, the classifier calculates the probability that it is spam and compares it to the probability of it being legitimate. The email is then classified as spam or not based on this comparison.\n",
    "   - Naïve Bayes is effective for spam filtering because it can handle a large number of email features efficiently, making it suitable for real-time email processing.\n",
    "\n",
    "### Market Sentiment Analysis:\n",
    "   - Market sentiment analysis involves assessing the sentiment or emotional tone of market-related content, such as news articles, social media posts, and financial reports.\n",
    "   - The Naïve Bayes classifier can be used to determine whether the overall sentiment is positive, negative, or neutral based on the sentiment expressed in the text.\n",
    "   - To do this, the classifier is trained on labeled data with examples of positive, negative, and neutral sentiment expressions. It estimates the conditional probabilities of certain words or phrases being associated with each sentiment category.\n",
    "   - When analyzing new market-related text, the classifier calculates the probabilities of it falling into different sentiment categories. This information can be valuable for traders, investors, and financial analysts in making informed decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5f5f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988c558b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f682ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aa1ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
