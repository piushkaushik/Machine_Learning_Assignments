{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e985cd2",
   "metadata": {},
   "source": [
    "## 1. What is the estimated depth of a Decision Tree trained (unrestricted) on a one million instance training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb5bb90",
   "metadata": {},
   "source": [
    "**Ans:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f577df2",
   "metadata": {},
   "source": [
    "The estimated depth of an unrestricted Decision Tree on a one million instance training set can vary widely and depends on several factors, including the complexity of the data and the specific algorithm or software used to train the tree. Theoretically, an unrestricted Decision Tree could grow to have a depth equal to the number of instances in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db37e55",
   "metadata": {},
   "source": [
    "## 2. Is the Gini impurity of a node usually lower or higher than that of its parent? Is it always lower/greater, or is it usually lower/greater?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41772d6",
   "metadata": {},
   "source": [
    "**Ans:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e70fc55",
   "metadata": {},
   "source": [
    "The Gini impurity of a node in a decision tree is usually lower than or equal to the Gini impurity of its parent node after a split. However, it is not an absolute rule and can depend on the specific dataset and split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902432f1",
   "metadata": {},
   "source": [
    "## 3. Explain if its a good idea to reduce max depth if a Decision Tree is overfitting the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7087fc",
   "metadata": {},
   "source": [
    "**Ans:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928765f0",
   "metadata": {},
   "source": [
    "Reducing the maximum depth of a decision tree can be a good idea if the tree is overfitting the training set. Overfitting occurs when the tree is too complex and captures noise in the data, making it perform poorly on unseen data (testing data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbee19c8",
   "metadata": {},
   "source": [
    "Benefits of reducing the max depth of a decosion tree:\n",
    "- Simplifies the Model\n",
    "- Avoids Overfitting\n",
    "- Improves Interpretability\n",
    "- Enhances Training Speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039a0e89",
   "metadata": {},
   "source": [
    "## 4. Explain if its a good idea to try scaling the input features if a Decision Tree underfits the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f153d48",
   "metadata": {},
   "source": [
    "**Ans**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9a8d47",
   "metadata": {},
   "source": [
    "Scaling the input features is generally not necessary or effective for addressing underfitting in a decision tree. Decision trees are a type of machine learning model that are not influenced by the scale of input features in the same way as some other algorithms, such as Support Vector Machines (SVMs) or k-Nearest Neighbors (k-NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e31041",
   "metadata": {},
   "source": [
    "## 5. How much time will it take to train another Decision Tree on a training set of 10 million instances if it takes an hour to train a Decision Tree on a training set with 1 million instances?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffda247",
   "metadata": {},
   "source": [
    "**Ans:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b66f3b",
   "metadata": {},
   "source": [
    "The time required to train a Decision Tree on a training set depends on various factors, and it's not always a linear relationship with the number of instances. However, if we assume that the training time scales linearly with the number of instances, we can make an estimate.\n",
    "\n",
    "If it takes 1 hour to train a Decision Tree on a training set with 1 million instances, and we want to train it on a training set with 10 million instances, the training time can be estimated as follows:\n",
    "\n",
    "Time to train on 10 million instances = (Time to train on 1 million instances) * (Number of instances to train on / 1 million)\n",
    "\n",
    "Time to train on 10 million instances = 1 hour * (10 million / 1 million)\n",
    "\n",
    "Time to train on 10 million instances = 1 hour * 10\n",
    "\n",
    "Time to train on 10 million instances = 10 hours\n",
    "\n",
    "So, if we assume a linear relationship, it would take approximately 10 hours to train a Decision Tree on a training set with 10 million instances, assuming similar hardware and settings. Keep in mind that this is a simplified estimate, and in practice, training times can vary due to factors like the complexity of the tree, hardware resources, and software optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e233e1",
   "metadata": {},
   "source": [
    "## 6. Will setting `presort = True` speed up training if your training set has `100,000` instances?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98349a1e",
   "metadata": {},
   "source": [
    "**Ans:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f57980",
   "metadata": {},
   "source": [
    "Setting `presort = True` in a Decision Tree can help speed up training for small to moderately sized datasets. Presorting involves sorting the data for each feature to find the best split points, which can be computationally expensive. However, it can speed up training for smaller datasets because it allows the algorithm to quickly identify good split points.\n",
    "\n",
    "\n",
    "In your case, with a training set of 100,000 instances, using `presort = True` could potentially speed up training, especially if the dataset is relatively small and can fit into memory. It precomputes feature values, which can make finding optimal splits faster.\n",
    "\n",
    "\n",
    "However, for very large datasets, the cost of presorting may outweigh the benefits, as sorting all features for each node becomes computationally expensive. In such cases, it's often better to set `presort = False` (which is the default) and let the algorithm choose the optimal strategy.\n",
    "\n",
    "\n",
    "The impact of setting `presort = True` on training time can vary depending on factors like the dataset size, hardware, and available memory, so it's a good practice to experiment with both settings to see which one performs better for your specific case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4377f1",
   "metadata": {},
   "source": [
    "## 7. Follow these steps to train and fine-tune a Decision Tree for the moons dataset:\n",
    "### a. To build a moons dataset, use make moons(n samples=10000, noise=0.4).\n",
    "### b. Divide the dataset into a training and a test collection with train test split().\n",
    "### c. To find good hyperparameters values for a DecisionTreeClassifier, use grid search with cross-validation (with the GridSearchCV class). Try different values for max leaf nodes.\n",
    "### d. Use these hyperparameters to train the model on the entire training set, and then assess its output on the test set. You can achieve an accuracy of 85 to 87 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be461086",
   "metadata": {},
   "source": [
    "**Ans:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8c674f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0095d1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create the moons dataset\n",
    "X, y = make_moons(n_samples=10000, noise=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee5d0f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Split the dataset into a training and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8226a75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define hyperparameters to search\n",
    "param_grid = {\n",
    "    'max_leaf_nodes': [None, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cae7e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Create a DecisionTreeClassifier\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34ac675d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=DecisionTreeClassifier(random_state=42), n_jobs=-1,\n",
       "             param_grid={&#x27;max_leaf_nodes&#x27;: [None, 10, 20, 30, 40, 50, 60, 70,\n",
       "                                            80, 90, 100]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=DecisionTreeClassifier(random_state=42), n_jobs=-1,\n",
       "             param_grid={&#x27;max_leaf_nodes&#x27;: [None, 10, 20, 30, 40, 50, 60, 70,\n",
       "                                            80, 90, 100]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=DecisionTreeClassifier(random_state=42), n_jobs=-1,\n",
       "             param_grid={'max_leaf_nodes': [None, 10, 20, 30, 40, 50, 60, 70,\n",
       "                                            80, 90, 100]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6: Use GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(tree_clf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73093c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Get the best hyperparameters\n",
    "best_params = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad76d369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_leaf_nodes=20, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_leaf_nodes=20, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_leaf_nodes=20, random_state=42)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 8: Train the model on the entire training set with the best hyperparameters\n",
    "best_tree_clf = DecisionTreeClassifier(**best_params, random_state=42)\n",
    "best_tree_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "427e8969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 87.00%\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Assess the model's performance on the test set\n",
    "y_pred = best_tree_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on the test set: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09b02e9",
   "metadata": {},
   "source": [
    "## 8. Follow these steps to grow a forest:\n",
    "\n",
    "### a. Using the same method as before, create 1,000 subsets of the training set, each containing 100 instances chosen at random. You can do this with Scikit-ShuffleSplit Learn&#39;s class.\n",
    "### b. Using the best hyperparameter values found in the previous exercise, train one Decision Tree on each subset. On the test collection, evaluate these 1,000 Decision Trees. These Decision Trees would likely perform worse than the first Decision Tree, achieving only around 80% accuracy, since they were trained on smaller sets.\n",
    "### c. Now the magic begins. Create 1,000 Decision Tree predictions for each test set case, and keep only the most common prediction (you can do this with SciPy&#39;s mode() function). Over the test collection, this method gives you majority-vote predictions.\n",
    "### d. On the test range, evaluate these predictions: you should achieve a slightly higher accuracy than the first model (approx 0.5 to 1.5 percent higher). You&#39;ve successfully learned a Random Forest classifier!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68c0faf",
   "metadata": {},
   "source": [
    "**Ans:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0755b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.base import clone\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f15b5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step a: Create 1,000 subsets of the training set\n",
    "n_trees = 1000\n",
    "n_instances = 100\n",
    "\n",
    "subset_estimators = []\n",
    "\n",
    "rs = ShuffleSplit(n_splits=n_trees, test_size=n_instances, random_state=42)\n",
    "\n",
    "for train_index, _ in rs.split(X_train):\n",
    "    subset_tree = RandomForestClassifier(n_estimators=10, max_leaf_nodes=16)  \n",
    "    subset_tree.fit(X_train[train_index], y_train[train_index])\n",
    "    subset_estimators.append(subset_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25903971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step b: Train Decision Trees on each subset and evaluate\n",
    "accuracy_scores = []\n",
    "for estimator in subset_estimators:\n",
    "    y_pred = estimator.predict(X_test)\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    accuracy_scores.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcc0c720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 87.15%\n"
     ]
    }
   ],
   "source": [
    "# Step c: Make predictions and take the majority vote\n",
    "forest_predictions = np.empty([n_trees, len(X_test)], dtype=np.uint8)\n",
    "for tree_index, estimator in enumerate(subset_estimators):\n",
    "    forest_predictions[tree_index] = estimator.predict(X_test)\n",
    "\n",
    "y_pred_majority, _ = mode(forest_predictions, axis=0)\n",
    "\n",
    "# Step d: Evaluate the Random Forest\n",
    "forest_accuracy = np.mean(y_pred_majority.ravel() == y_test)\n",
    "print(\"Random Forest Accuracy: {:.2f}%\".format(forest_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3a8283",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
